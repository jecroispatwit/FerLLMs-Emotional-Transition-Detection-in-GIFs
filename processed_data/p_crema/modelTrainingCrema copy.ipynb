{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import DataParallel  # For multi-GPU support\n",
    "from torch.cuda.amp import GradScaler, autocast  # For mixed precision training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Vision Transformer\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights  # ViT model and pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a pre-trained Vision Transformer model\n",
    "class EmotionRecognitionViT(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(EmotionRecognitionViT, self).__init__()\n",
    "        # Load the pre-trained Vision Transformer\n",
    "        self.vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Replace the classifier head with one suitable for our task\n",
    "        in_features = self.vit.heads[0].in_features  # Access the input features of the last layer\n",
    "        self.vit.heads = nn.Sequential(nn.Linear(in_features, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "# Initialize the ViT model\n",
    "model = EmotionRecognitionViT(num_classes=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data  # Preprocessed frames\n",
    "        self.labels = labels  # Corresponding emotion labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file and GIF folder\n",
    "csv_path = \"/home/jecroisp/Thesis/processed_data/p_crema/GIF_Annotations.csv\"\n",
    "gif_folder = \"/home/jecroisp/Thesis/processed_data/p_crema/CremaGifs\"\n",
    "\n",
    "# Load the CSV\n",
    "metadata = pd.read_csv(csv_path)\n",
    "\n",
    "# Filter valid files and map paths\n",
    "metadata['GIF_Path'] = metadata['fileName'].apply(lambda x: os.path.join(gif_folder, f\"{x}.gif\"))\n",
    "metadata = metadata[metadata['GIF_Path'].apply(os.path.exists)].reset_index(drop=True)\n",
    "\n",
    "# Map emotion labels (emoVote) to integers\n",
    "emotion_mapping = {\n",
    "    \"A\": 0,  # Anger\n",
    "    \"D\": 1,  # Disgust\n",
    "    \"F\": 2,  # Fear\n",
    "    \"H\": 3,  # Happiness\n",
    "    \"N\": 4,  # Neutral\n",
    "    \"S\": 5   # Sadness\n",
    "}\n",
    "metadata['Emotion_Label'] = metadata['emoVote'].map(emotion_mapping)\n",
    "\n",
    "# Optional: Filter out low-agreement samples\n",
    "metadata = metadata[metadata['agreement'] >= 0.6].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIFDataset(Dataset):\n",
    "    def __init__(self, metadata, transform=None, max_frames=10):\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gif_path = self.metadata.loc[idx, 'GIF_Path']\n",
    "        label = self.metadata.loc[idx, 'Emotion_Label']\n",
    "\n",
    "        # Load GIF as a sequence of frames\n",
    "        gif = Image.open(gif_path)\n",
    "        frames = []\n",
    "        try:\n",
    "            while True:\n",
    "                frame = gif.copy().convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "                gif.seek(gif.tell() + 1)\n",
    "        except EOFError:\n",
    "            pass\n",
    "\n",
    "        # If too many frames, sample evenly\n",
    "        if len(frames) > self.max_frames:\n",
    "            indices = torch.linspace(0, len(frames) - 1, self.max_frames).long()\n",
    "            frames = [frames[i] for i in indices]\n",
    "\n",
    "        # Aggregate frames (e.g., average pooling across frames)\n",
    "        frames_tensor = torch.stack(frames, dim=0)  # Shape: [num_frames, channels, height, width]\n",
    "        aggregated_tensor = frames_tensor.mean(dim=0)  # Shape: [channels, height, width]\n",
    "\n",
    "        return aggregated_tensor, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define transformations for GIF frames\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize frames to 224x224\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Create the dataset and DataLoader\n",
    "dataset = GIFDataset(metadata, transform=transform)\n",
    "batch_size_per_gpu = 16  # Adjust based on memory capacity\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size_per_gpu * torch.cuda.device_count(), shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple GPUs available. Using only GPU 0 for debugging.\n"
     ]
    }
   ],
   "source": [
    "# # Set device and wrap model for multi-GPU training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Multiple GPUs available. Using only GPU 0 for debugging.\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1569886/2435396245.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)  # Lower learning rate for fine-tuning ViT\n",
    "\n",
    "# Initialize the GradScaler for mixed precision\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1569886/786922547.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.2780\n",
      "Epoch 2/10, Loss: 0.8405\n",
      "Epoch 3/10, Loss: 0.6777\n",
      "Epoch 4/10, Loss: 0.5773\n",
      "Epoch 5/10, Loss: 0.5119\n",
      "Epoch 6/10, Loss: 0.4653\n",
      "Epoch 7/10, Loss: 0.4308\n",
      "Epoch 8/10, Loss: 0.4017\n",
      "Epoch 9/10, Loss: 0.3893\n",
      "Epoch 10/10, Loss: 0.3642\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for frames, labels in dataloader:\n",
    "        # Move data to the appropriate device\n",
    "        frames = frames.to(device, dtype=torch.float32)  # Ensure frames are float32\n",
    "        labels = labels.to(device, dtype=torch.long)    # Ensure labels are long (int)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with autocast():\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)  # CrossEntropyLoss expects float32 and long\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save(model.state_dict(), \"emotion_recognition_vit.pth\")\n",
    "\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"/home/jecroisp/Thesis/processed_data/p_crema/emotion_recognition_vit.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in dataloader:\n",
    "            # Move data to the appropriate device\n",
    "            frames = frames.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(frames)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test DataLoader\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset)  # No shuffling for test\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, sampler=test_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_dataloader, device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Accuracy on Test Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, dataloader, device):\n\u001b[0;32m----> 2\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "accuracy = evaluate_model(model, test_dataloader, device)\n",
    "print(f\"Model Accuracy on Test Dataset: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model_per_class(model, dataloader, device, num_classes):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    class_correct = np.zeros(num_classes)\n",
    "    class_total = np.zeros(num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in dataloader:\n",
    "            # Move data to the appropriate device\n",
    "            frames = frames.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(frames)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_correct[label] += (predicted[i] == label).item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    # Print per-class accuracy\n",
    "    for i in range(num_classes):\n",
    "        accuracy = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "        print(f\"Accuracy of class {i}: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of class 0: 88.34%\n",
      "Accuracy of class 1: 95.51%\n",
      "Accuracy of class 2: 87.17%\n",
      "Accuracy of class 3: 99.12%\n",
      "Accuracy of class 4: 83.69%\n",
      "Accuracy of class 5: 65.80%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_per_class(model, test_dataloader, device, num_classes=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54749/3196082762.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"emotion_recognition_vit.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmotionRecognitionViT(\n",
       "  (vit): VisionTransformer(\n",
       "    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (encoder): Encoder(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): Sequential(\n",
       "        (encoder_layer_0): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_1): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_2): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_3): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_4): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_5): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_6): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_7): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_8): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_9): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_10): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_11): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (heads): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model architecture\n",
    "model = EmotionRecognitionViT(num_classes=6)\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load(\"emotion_recognition_vit.pth\"))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the same transformations used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize frames\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "def preprocess_gif(gif_path, max_frames=10):\n",
    "    gif = Image.open(gif_path)\n",
    "    frames = []\n",
    "\n",
    "    # Extract frames and apply transformations\n",
    "    try:\n",
    "        while True:\n",
    "            frame = gif.copy().convert(\"RGB\")  # Convert frame to RGB\n",
    "            frame = transform(frame)\n",
    "            frames.append(frame)\n",
    "            gif.seek(gif.tell() + 1)  # Move to the next frame\n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "    # # Sample or pad frames to max_frames\n",
    "    # if len(frames) > max_frames:\n",
    "    #     indices = torch.linspace(0, len(frames) - 1, steps=max_frames).long()\n",
    "    #     frames = [frames[i] for i in indices]\n",
    "    # elif len(frames) < max_frames:\n",
    "    #     padding = [torch.zeros_like(frames[0]) for _ in range(max_frames - len(frames))]\n",
    "    #     frames.extend(padding)\n",
    "    \n",
    "    frames_tensor = torch.stack(frames)  # Shape: [num_frames, 3, 224, 224]\n",
    "    aggregated_tensor = frames_tensor.mean(dim=0)  # Shape: [3, 224, 224]\n",
    "\n",
    "    # Stack frames into a single tensor\n",
    "    # return torch.stack(frames)  # Shape: [max_frames, 3, 224, 224]\n",
    "    return aggregated_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion: Anger\n"
     ]
    }
   ],
   "source": [
    "# Path to the unseen GIF\n",
    "unseen_gif_path = \"/home/jecroisp/Thesis/processed_data/p_crema/rant-black.gif\"\n",
    "\n",
    "# Preprocess the GIF\n",
    "frames_tensor = preprocess_gif(unseen_gif_path).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "# Predict emotion\n",
    "with torch.no_grad():\n",
    "    outputs = model(frames_tensor)  # Forward pass\n",
    "    _, predicted = torch.max(outputs, 1)  # Get the predicted class\n",
    "    predicted_class = predicted.item()\n",
    "\n",
    "# Map the predicted class to the corresponding emotion\n",
    "emotion_mapping = {\n",
    "    0: \"Anger\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happiness\",\n",
    "    4: \"Neutral\",\n",
    "    5: \"Sadness\"\n",
    "}\n",
    "print(f\"Predicted Emotion: {emotion_mapping[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion: Anger (Confidence: 61.01%)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Predict emotion\n",
    "with torch.no_grad():\n",
    "    outputs = model(frames_tensor)  # Forward pass\n",
    "    probabilities = F.softmax(outputs, dim=1)  # Convert logits to probabilities\n",
    "    confidence, predicted_class = torch.max(probabilities, 1)  # Get confidence and predicted class\n",
    "\n",
    "# Map the predicted class to the corresponding emotion\n",
    "emotion_mapping = {\n",
    "    0: \"Anger\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happiness\",\n",
    "    4: \"Neutral\",\n",
    "    5: \"Sadness\"\n",
    "}\n",
    "\n",
    "predicted_emotion = emotion_mapping[predicted_class.item()]\n",
    "confidence_score = confidence.item() * 100  # Convert to percentage\n",
    "print(f\"Predicted Emotion: {predicted_emotion} (Confidence: {confidence_score:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Scores:\n",
      "Anger: 61.01%\n",
      "Disgust: 7.34%\n",
      "Fear: 3.02%\n",
      "Happiness: 4.32%\n",
      "Neutral: 23.40%\n",
      "Sadness: 0.90%\n"
     ]
    }
   ],
   "source": [
    "# Print confidence scores for all classes\n",
    "print(\"Confidence Scores:\")\n",
    "for i, prob in enumerate(probabilities.squeeze(0)):  # Remove batch dimension\n",
    "    print(f\"{emotion_mapping[i]}: {prob.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
